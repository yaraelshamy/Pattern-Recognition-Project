{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtWidgets , uic\n",
    "from PyQt5.QtWidgets import QGraphicsScene, QGraphicsView, QMessageBox\n",
    "from PyQt5.QtGui import QPixmap\n",
    "from PIL import Image\n",
    "import PIL.ImageQt\n",
    "from gui import Ui_MainWindow\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras as ks\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    \"\"\"\n",
    "    Loads each image in the given folder and save it in an array.\n",
    "    \n",
    "    In case of training and validation data, it saves 3 copies of images:\n",
    "    original, flipped and rotated copies.\n",
    "    \n",
    "    Returns array of images and array of their labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = 0\n",
    "    flag = 1\n",
    "    for file_name in os.listdir(folder):\n",
    "        if folder == 'Testing': flag = 0 \n",
    "        sub_dir_path = folder + '/' + file_name\n",
    "        for filename in os.listdir(sub_dir_path):     \n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(sub_dir_path, filename), cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, (28, 28))\n",
    "                flipped_img = cv2.flip(img, 0)\n",
    "                rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(' ')\n",
    "            \n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(classes)\n",
    "                if flag:\n",
    "                    images.append(flipped_img)\n",
    "                    images.append(rotated_img)\n",
    "                    labels.append(classes)\n",
    "                    labels.append(classes)\n",
    "\n",
    "        classes += 1\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "1818 375 131\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_images_from_folder(\"Training\")\n",
    "validation_images, validation_labels = load_images_from_folder(\"Validation\")\n",
    "test_images, test_labels = load_images_from_folder(\"Testing\")\n",
    "class_names = np.array([\"actin\", \"dna\", \"endosome\", \"er\", \"golgia\", \"golgpp\", \"lysosome\", \"microtubulus\", \"mitochondria\", \"nucleolus\"])\n",
    "print(len(train_images),len(validation_images), len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:(1818, 28, 28) \n",
      "No. of Training Dataset Labels:1818 \n",
      "Validation Dataset Shape:(375, 28, 28) \n",
      "No. of Validation Dataset Labels:375 \n",
      "Test Dataset Shape:(131, 28, 28) \n",
      "No. of Test Dataset Labels:131 \n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset Shape:{} '.format(train_images.shape)) \n",
    "print('No. of Training Dataset Labels:{} '.format(len(train_labels))) \n",
    "print('Validation Dataset Shape:{} '.format(validation_images.shape))\n",
    "print('No. of Validation Dataset Labels:{} '.format(len(validation_labels)))\n",
    "print('Test Dataset Shape:{} '.format(test_images.shape))\n",
    "print('No. of Test Dataset Labels:{} '.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221,\n",
       "       222, 224, 226, 227, 229, 230, 231, 236, 237, 239, 240, 241, 247],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0 \n",
    "validation_images = validation_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape arrays to 4 dims to pass them to our model\n",
    "\n",
    "train_images = train_images.reshape((1818, 28, 28, 1))\n",
    "validation_images = validation_images.reshape((375, 28, 28, 1))\n",
    "test_images = test_images.reshape((131, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:(1818, 28, 28, 1) \n",
      "No. of Training Dataset Labels:1818 \n",
      "Validation Dataset Shape:(375, 28, 28, 1) \n",
      "No. of Validation Dataset Labels:375 \n",
      "Test Dataset Shape:(131, 28, 28, 1) \n",
      "No. of Test Dataset Labels:131 \n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset Shape:{} '.format(train_images.shape)) \n",
    "print('No. of Training Dataset Labels:{} '.format(len(train_labels)))\n",
    "print('Validation Dataset Shape:{} '.format(validation_images.shape))\n",
    "print('No. of Validation Dataset Labels:{} '.format(len(validation_labels)))\n",
    "print('Test Dataset Shape:{} '.format(test_images.shape))\n",
    "print('No. of Test Dataset Labels:{} '.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = ks.models.Sequential()\n",
    "cnn_model.add(ks.layers.Conv2D(64, (5,5), activation= 'relu', input_shape= (28, 28, 1), name='Convolutional_layer_1'))\n",
    "cnn_model.add(ks.layers.Conv2D(64, (5,5), activation= 'relu', name='Convolutional_layer_2'))\n",
    "cnn_model.add(ks.layers.Conv2D(128, (3,3), activation= 'relu', name='Convolutional_layer_3'))\n",
    "\n",
    "cnn_model.add(ks.layers.MaxPooling2D((2,2), name= 'Maxpooling_2D_1'))\n",
    "cnn_model.add(ks.layers.MaxPooling2D((2,2), name= 'Maxpooling_2D_2'))\n",
    "cnn_model.add(ks.layers.MaxPooling2D((2,2), name= 'Maxpooling_2D_3'))\n",
    "\n",
    "cnn_model.add(ks.layers.Flatten(name= 'Flatten'))\n",
    "cnn_model.add(ks.layers.Dense(128, activation='relu', name='Hidden_layer'))\n",
    "cnn_model.add(ks.layers.Dense(10, activation='softmax', name='Output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Convolutional_layer_1 (Conv2 (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "Convolutional_layer_2 (Conv2 (None, 20, 20, 64)        102464    \n",
      "_________________________________________________________________\n",
      "Convolutional_layer_3 (Conv2 (None, 18, 18, 128)       73856     \n",
      "_________________________________________________________________\n",
      "Maxpooling_2D_1 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Maxpooling_2D_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "Maxpooling_2D_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_layer (Dense)         (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "Output_layer (Dense)         (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 244,938\n",
      "Trainable params: 244,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer= 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 1.9470 - accuracy: 0.2338 - val_loss: 1.6256 - val_accuracy: 0.3573\n",
      "Epoch 2/40\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 1.3416 - accuracy: 0.4488 - val_loss: 1.4326 - val_accuracy: 0.4400\n",
      "Epoch 3/40\n",
      "57/57 [==============================] - 6s 106ms/step - loss: 1.2102 - accuracy: 0.5143 - val_loss: 1.3323 - val_accuracy: 0.5573\n",
      "Epoch 4/40\n",
      "57/57 [==============================] - 6s 102ms/step - loss: 1.0782 - accuracy: 0.5688 - val_loss: 1.3241 - val_accuracy: 0.5147\n",
      "Epoch 5/40\n",
      "57/57 [==============================] - 6s 104ms/step - loss: 1.0058 - accuracy: 0.5748 - val_loss: 1.1742 - val_accuracy: 0.5760\n",
      "Epoch 6/40\n",
      "57/57 [==============================] - 6s 104ms/step - loss: 0.8658 - accuracy: 0.6672 - val_loss: 1.1423 - val_accuracy: 0.6187\n",
      "Epoch 7/40\n",
      "57/57 [==============================] - 6s 108ms/step - loss: 0.8119 - accuracy: 0.6744 - val_loss: 1.1002 - val_accuracy: 0.6027\n",
      "Epoch 8/40\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 0.7362 - accuracy: 0.7112 - val_loss: 1.0855 - val_accuracy: 0.6533\n",
      "Epoch 9/40\n",
      "57/57 [==============================] - 6s 109ms/step - loss: 0.6737 - accuracy: 0.7437 - val_loss: 0.9866 - val_accuracy: 0.7040\n",
      "Epoch 10/40\n",
      "57/57 [==============================] - 6s 109ms/step - loss: 0.6528 - accuracy: 0.7459 - val_loss: 1.1576 - val_accuracy: 0.6027\n",
      "Epoch 11/40\n",
      "57/57 [==============================] - 6s 112ms/step - loss: 0.6203 - accuracy: 0.7558 - val_loss: 0.9319 - val_accuracy: 0.6960\n",
      "Epoch 12/40\n",
      "57/57 [==============================] - 6s 112ms/step - loss: 0.5121 - accuracy: 0.8075 - val_loss: 0.9640 - val_accuracy: 0.6933\n",
      "Epoch 13/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.4977 - accuracy: 0.8168 - val_loss: 1.0739 - val_accuracy: 0.6800\n",
      "Epoch 14/40\n",
      "57/57 [==============================] - 6s 105ms/step - loss: 0.4633 - accuracy: 0.8251 - val_loss: 0.9310 - val_accuracy: 0.7093\n",
      "Epoch 15/40\n",
      "57/57 [==============================] - 6s 111ms/step - loss: 0.4881 - accuracy: 0.8141 - val_loss: 0.9221 - val_accuracy: 0.7120\n",
      "Epoch 16/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.4177 - accuracy: 0.8427 - val_loss: 1.0727 - val_accuracy: 0.6907\n",
      "Epoch 17/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.4100 - accuracy: 0.8416 - val_loss: 1.0120 - val_accuracy: 0.7147\n",
      "Epoch 18/40\n",
      "57/57 [==============================] - 6s 113ms/step - loss: 0.3717 - accuracy: 0.8575 - val_loss: 1.1764 - val_accuracy: 0.7067\n",
      "Epoch 19/40\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 0.3158 - accuracy: 0.8850 - val_loss: 1.2487 - val_accuracy: 0.7093\n",
      "Epoch 20/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.3099 - accuracy: 0.8779 - val_loss: 1.1995 - val_accuracy: 0.6960\n",
      "Epoch 21/40\n",
      "57/57 [==============================] - 6s 114ms/step - loss: 0.3020 - accuracy: 0.8856 - val_loss: 1.1299 - val_accuracy: 0.7093\n",
      "Epoch 22/40\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 0.3014 - accuracy: 0.8878 - val_loss: 0.9664 - val_accuracy: 0.7440\n",
      "Epoch 23/40\n",
      "57/57 [==============================] - 6s 108ms/step - loss: 0.2593 - accuracy: 0.9070 - val_loss: 1.2731 - val_accuracy: 0.6747\n",
      "Epoch 24/40\n",
      "57/57 [==============================] - 6s 104ms/step - loss: 0.2728 - accuracy: 0.8922 - val_loss: 1.0848 - val_accuracy: 0.7280\n",
      "Epoch 25/40\n",
      "57/57 [==============================] - 7s 114ms/step - loss: 0.2406 - accuracy: 0.9103 - val_loss: 1.3599 - val_accuracy: 0.6853\n",
      "Epoch 26/40\n",
      "57/57 [==============================] - 6s 111ms/step - loss: 0.2387 - accuracy: 0.9076 - val_loss: 1.2310 - val_accuracy: 0.7200\n",
      "Epoch 27/40\n",
      "57/57 [==============================] - 6s 108ms/step - loss: 0.1826 - accuracy: 0.9345 - val_loss: 1.1976 - val_accuracy: 0.7200\n",
      "Epoch 28/40\n",
      "57/57 [==============================] - 6s 112ms/step - loss: 0.1592 - accuracy: 0.9439 - val_loss: 1.3640 - val_accuracy: 0.6773\n",
      "Epoch 29/40\n",
      "57/57 [==============================] - 7s 115ms/step - loss: 0.1902 - accuracy: 0.9323 - val_loss: 1.1887 - val_accuracy: 0.7307\n",
      "Epoch 30/40\n",
      "57/57 [==============================] - 6s 111ms/step - loss: 0.1673 - accuracy: 0.9373 - val_loss: 1.4181 - val_accuracy: 0.6773\n",
      "Epoch 31/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.1576 - accuracy: 0.9461 - val_loss: 1.3675 - val_accuracy: 0.7200\n",
      "Epoch 32/40\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 0.1245 - accuracy: 0.9549 - val_loss: 1.4844 - val_accuracy: 0.7093\n",
      "Epoch 33/40\n",
      "57/57 [==============================] - 6s 112ms/step - loss: 0.1116 - accuracy: 0.9593 - val_loss: 1.3906 - val_accuracy: 0.7333\n",
      "Epoch 34/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.1984 - accuracy: 0.9323 - val_loss: 1.6485 - val_accuracy: 0.7067\n",
      "Epoch 35/40\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.1596 - accuracy: 0.9411 - val_loss: 1.3136 - val_accuracy: 0.7387\n",
      "Epoch 36/40\n",
      "57/57 [==============================] - 6s 113ms/step - loss: 0.0943 - accuracy: 0.9719 - val_loss: 1.5636 - val_accuracy: 0.7067\n",
      "Epoch 37/40\n",
      "57/57 [==============================] - 6s 108ms/step - loss: 0.0625 - accuracy: 0.9829 - val_loss: 1.5412 - val_accuracy: 0.7120\n",
      "Epoch 38/40\n",
      "57/57 [==============================] - 6s 113ms/step - loss: 0.0573 - accuracy: 0.9829 - val_loss: 1.6564 - val_accuracy: 0.6960\n",
      "Epoch 39/40\n",
      "57/57 [==============================] - 6s 111ms/step - loss: 0.0819 - accuracy: 0.9708 - val_loss: 1.4983 - val_accuracy: 0.7573\n",
      "Epoch 40/40\n",
      "57/57 [==============================] - 6s 108ms/step - loss: 0.0915 - accuracy: 0.9670 - val_loss: 1.5200 - val_accuracy: 0.7547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20d969a2970>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_images, train_labels, epochs= 40, validation_data= (validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0604 - accuracy: 0.9796\n",
      "Train Accuracy0.98 \n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = cnn_model.evaluate(train_images, train_labels) \n",
    "print('Train Accuracy{} '.format(round(float(train_accuracy), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 21ms/step - loss: 1.5200 - accuracy: 0.7547\n",
      "Validation Accuracy0.75 \n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_accuracy = cnn_model.evaluate(validation_images, validation_labels) \n",
    "print('Validation Accuracy{} '.format(round(float(validation_accuracy), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7961 - accuracy: 0.6641\n",
      "Test Accuracy 0.66 \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = cnn_model.evaluate(test_images, test_labels)\n",
    "print('Test Accuracy {} '.format(round(float(test_accuracy), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, display1, display2):\n",
    "    \"\"\"\n",
    "    Reads an image from testing dataset, and predict its label.\n",
    "    \n",
    "    Returns the image label and testing accuracy. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = np.array([img])\n",
    "\n",
    "    img =img.reshape(1, 28, 28, 1)\n",
    "    predictions = cnn_model.predict(img)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    \n",
    "    display1.setText(\"This image most likely belongs to {}\".format(class_names[np.argmax(predictions)]))\n",
    "    display2.setText(\"Test Accuracy: {} %\".format(round(float(test_accuracy * 100), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplicationWindow(QtWidgets.QMainWindow):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ApplicationWindow, self).__init__()\n",
    "        self.ui = Ui_MainWindow()\n",
    "        self.ui.setupUi(self)\n",
    "        \n",
    "        self.ui.browse_pushButton.clicked.connect(lambda: self.browse(self.ui.org_img_disp_label))\n",
    "        \n",
    "        \n",
    "    def browse(self, Ui_MainWindow):\n",
    "        \"\"\"\n",
    "        Uploads an image, resizes it for prediction,\n",
    "        then resizes it again for plotting. \n",
    "        \"\"\"\n",
    "\n",
    "        filename = QtWidgets.QFileDialog.getOpenFileName(self, 'Open file', \"*.tif\")\n",
    "        if (filename == ([], '')) | (filename ==  0 ):\n",
    "            return\n",
    "        imgPath = str(filename[0])\n",
    "        try:\n",
    "            img = cv2.imread(imgPath, cv2.IMREAD_GRAYSCALE)\n",
    "            img_test = cv2.resize(img, (28, 28))\n",
    "            img_show = cv2.resize(img, (721, 401))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(' ')\n",
    "\n",
    "        self.showImage(img_show, Ui_MainWindow)\n",
    "        predict_image(img_test, self.ui.result_label, self.ui.result_label_2)\n",
    "        \n",
    "        \n",
    "    def showImage(self, arr, Ui_MainWindow):\n",
    "        \"\"\"\n",
    "        Convert an array to image and display it given image handler\n",
    "        \"\"\"\n",
    "\n",
    "        imgBack = Image.fromarray(arr)\n",
    "        qimage = PIL.ImageQt.ImageQt(imgBack)\n",
    "        pixmap = QtGui.QPixmap.fromImage(qimage) \n",
    "        Ui_MainWindow.setPixmap(pixmap)\n",
    "        Ui_MainWindow.setMask(pixmap.mask())\n",
    "        Ui_MainWindow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    application = ApplicationWindow()\n",
    "    application.show()\n",
    "    app.exec_()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
