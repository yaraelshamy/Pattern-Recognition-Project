{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, QtGui, QtWidgets , uic\n",
    "from PyQt5.QtWidgets import QGraphicsScene, QGraphicsView, QMessageBox\n",
    "from PyQt5.QtGui import QPixmap\n",
    "from PIL import Image\n",
    "import PIL.ImageQt\n",
    "from gui import Ui_MainWindow\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras as ks\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    \"\"\"\n",
    "    Loads each image in the given folder and save it in an array.\n",
    "    \n",
    "    In case of training and validation data, it saves 3 copies of images:\n",
    "    original, flipped and rotated copies.\n",
    "    \n",
    "    Returns array of images and array of their labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = 0\n",
    "    flag = 1\n",
    "    for file_name in os.listdir(folder):\n",
    "        if folder == 'Testing': flag = 0 \n",
    "        sub_dir_path = folder + '/' + file_name\n",
    "        for filename in os.listdir(sub_dir_path):     \n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(sub_dir_path, filename), cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, (28, 28))\n",
    "                flipped_img = cv2.flip(img, 0)\n",
    "                rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(' ')\n",
    "            \n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(classes)\n",
    "                if flag:\n",
    "                    images.append(flipped_img)\n",
    "                    images.append(rotated_img)\n",
    "                    labels.append(classes)\n",
    "                    labels.append(classes)\n",
    "\n",
    "        classes += 1\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "1818 375 131\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_images_from_folder(\"Training\")\n",
    "validation_images, validation_labels = load_images_from_folder(\"Validation\")\n",
    "test_images, test_labels = load_images_from_folder(\"Testing\")\n",
    "class_names = np.array([\"actin\", \"dna\", \"endosome\", \"er\", \"golgia\", \"golgpp\", \"lysosome\", \"microtubulus\", \"mitochondria\", \"nucleolus\"])\n",
    "print(len(train_images),len(validation_images), len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:(1818, 28, 28) \n",
      "No. of Training Dataset Labels:1818 \n",
      "Validation Dataset Shape:(375, 28, 28) \n",
      "No. of Validation Dataset Labels:375 \n",
      "Test Dataset Shape:(131, 28, 28) \n",
      "No. of Test Dataset Labels:131 \n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset Shape:{} '.format(train_images.shape)) \n",
    "print('No. of Training Dataset Labels:{} '.format(len(train_labels))) \n",
    "print('Validation Dataset Shape:{} '.format(validation_images.shape))\n",
    "print('No. of Validation Dataset Labels:{} '.format(len(validation_labels)))\n",
    "print('Test Dataset Shape:{} '.format(test_images.shape))\n",
    "print('No. of Test Dataset Labels:{} '.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221,\n",
       "       222, 224, 226, 227, 229, 230, 231, 236, 237, 239, 240, 241, 247],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0 \n",
    "validation_images = validation_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape arrays to 4 dims to pass them to our model\n",
    "\n",
    "train_images = train_images.reshape((1818, 28, 28, 1))\n",
    "validation_images = validation_images.reshape((375, 28, 28, 1))\n",
    "test_images = test_images.reshape((131, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:(1818, 28, 28, 1) \n",
      "No. of Training Dataset Labels:1818 \n",
      "Validation Dataset Shape:(375, 28, 28, 1) \n",
      "No. of Validation Dataset Labels:375 \n",
      "Test Dataset Shape:(131, 28, 28, 1) \n",
      "No. of Test Dataset Labels:131 \n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset Shape:{} '.format(train_images.shape)) \n",
    "print('No. of Training Dataset Labels:{} '.format(len(train_labels)))\n",
    "print('Validation Dataset Shape:{} '.format(validation_images.shape))\n",
    "print('No. of Validation Dataset Labels:{} '.format(len(validation_labels)))\n",
    "print('Test Dataset Shape:{} '.format(test_images.shape))\n",
    "print('No. of Test Dataset Labels:{} '.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = ks.models.Sequential()\n",
    "cnn_model.add(ks.layers.Conv2D(64, (5,5), activation= 'relu', input_shape= (28, 28, 1), name='Convolutional_layer_1'))\n",
    "cnn_model.add(ks.layers.Conv2D(64, (5,5), activation= 'relu', name='Convolutional_layer_2'))\n",
    "cnn_model.add(ks.layers.Conv2D(128, (3,3), activation= 'relu', name='Convolutional_layer_3'))\n",
    "\n",
    "cnn_model.add(ks.layers.MaxPooling2D((2,2), name= 'Maxpooling_2D_1'))\n",
    "cnn_model.add(ks.layers.MaxPooling2D((2,2), name= 'Maxpooling_2D_2'))\n",
    "cnn_model.add(ks.layers.MaxPooling2D((2,2), name= 'Maxpooling_2D_3'))\n",
    "\n",
    "cnn_model.add(ks.layers.Flatten(name= 'Flatten'))\n",
    "cnn_model.add(ks.layers.Dense(128, activation='relu', name='Hidden_layer'))\n",
    "cnn_model.add(ks.layers.Dense(10, activation='softmax', name='Output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Convolutional_layer_1 (Conv2 (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "Convolutional_layer_2 (Conv2 (None, 20, 20, 64)        102464    \n",
      "_________________________________________________________________\n",
      "Convolutional_layer_3 (Conv2 (None, 18, 18, 128)       73856     \n",
      "_________________________________________________________________\n",
      "Maxpooling_2D_1 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Maxpooling_2D_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "Maxpooling_2D_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_layer (Dense)         (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "Output_layer (Dense)         (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 244,938\n",
      "Trainable params: 244,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer= 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1818 samples, validate on 375 samples\n",
      "Epoch 1/40\n",
      "1818/1818 [==============================] - 10s 6ms/sample - loss: 1.9384 - acc: 0.2327 - val_loss: 1.6281 - val_acc: 0.3387\n",
      "Epoch 2/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 1.4090 - acc: 0.4334 - val_loss: 1.3926 - val_acc: 0.4667\n",
      "Epoch 3/40\n",
      "1818/1818 [==============================] - 10s 6ms/sample - loss: 1.1766 - acc: 0.5138 - val_loss: 1.3134 - val_acc: 0.4880\n",
      "Epoch 4/40\n",
      "1818/1818 [==============================] - 10s 6ms/sample - loss: 1.0712 - acc: 0.5622 - val_loss: 1.2774 - val_acc: 0.5120\n",
      "Epoch 5/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.9766 - acc: 0.6056 - val_loss: 1.2019 - val_acc: 0.5947\n",
      "Epoch 6/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.9475 - acc: 0.6309 - val_loss: 1.1132 - val_acc: 0.5920\n",
      "Epoch 7/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.7979 - acc: 0.6947 - val_loss: 1.1360 - val_acc: 0.5947\n",
      "Epoch 8/40\n",
      "1818/1818 [==============================] - 10s 6ms/sample - loss: 0.7294 - acc: 0.7244 - val_loss: 1.1143 - val_acc: 0.6347\n",
      "Epoch 9/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.6799 - acc: 0.7310 - val_loss: 0.9008 - val_acc: 0.6773\n",
      "Epoch 10/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.6354 - acc: 0.7651 - val_loss: 0.9385 - val_acc: 0.6880\n",
      "Epoch 11/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.6365 - acc: 0.7563 - val_loss: 0.9281 - val_acc: 0.6613\n",
      "Epoch 12/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.5284 - acc: 0.7976 - val_loss: 1.1054 - val_acc: 0.6560\n",
      "Epoch 13/40\n",
      "1818/1818 [==============================] - 10s 6ms/sample - loss: 0.5469 - acc: 0.7871 - val_loss: 0.9092 - val_acc: 0.6987\n",
      "Epoch 14/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.4677 - acc: 0.8179 - val_loss: 1.0437 - val_acc: 0.6800\n",
      "Epoch 15/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.4571 - acc: 0.8278 - val_loss: 0.8687 - val_acc: 0.7307\n",
      "Epoch 16/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.3993 - acc: 0.8520 - val_loss: 1.0859 - val_acc: 0.6933\n",
      "Epoch 17/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.3854 - acc: 0.8564 - val_loss: 0.9643 - val_acc: 0.7173\n",
      "Epoch 18/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.4079 - acc: 0.8493 - val_loss: 1.1530 - val_acc: 0.7147\n",
      "Epoch 19/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.3384 - acc: 0.8713 - val_loss: 1.1481 - val_acc: 0.7173\n",
      "Epoch 20/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.3836 - acc: 0.8531 - val_loss: 0.9469 - val_acc: 0.7227\n",
      "Epoch 21/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.3338 - acc: 0.8724 - val_loss: 0.9675 - val_acc: 0.7227\n",
      "Epoch 22/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.2617 - acc: 0.9054 - val_loss: 1.0082 - val_acc: 0.7173\n",
      "Epoch 23/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.2552 - acc: 0.9048 - val_loss: 1.1562 - val_acc: 0.7387\n",
      "Epoch 24/40\n",
      "1818/1818 [==============================] - 10s 6ms/sample - loss: 0.2600 - acc: 0.9010 - val_loss: 1.2164 - val_acc: 0.7147\n",
      "Epoch 25/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.2345 - acc: 0.9131 - val_loss: 1.0377 - val_acc: 0.7253\n",
      "Epoch 26/40\n",
      "1818/1818 [==============================] - 9s 5ms/sample - loss: 0.2083 - acc: 0.9180 - val_loss: 1.0974 - val_acc: 0.7147\n",
      "Epoch 27/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.2013 - acc: 0.9213 - val_loss: 1.2116 - val_acc: 0.7200\n",
      "Epoch 28/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.1725 - acc: 0.9340 - val_loss: 1.2003 - val_acc: 0.7413\n",
      "Epoch 29/40\n",
      "1818/1818 [==============================] - 9s 5ms/sample - loss: 0.1997 - acc: 0.9323 - val_loss: 1.3443 - val_acc: 0.7280\n",
      "Epoch 30/40\n",
      "1818/1818 [==============================] - 9s 5ms/sample - loss: 0.1837 - acc: 0.9345 - val_loss: 1.1777 - val_acc: 0.7440\n",
      "Epoch 31/40\n",
      "1818/1818 [==============================] - 11s 6ms/sample - loss: 0.1766 - acc: 0.9318 - val_loss: 1.2380 - val_acc: 0.7227\n",
      "Epoch 32/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.1196 - acc: 0.9593 - val_loss: 1.2983 - val_acc: 0.7573\n",
      "Epoch 33/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.1028 - acc: 0.9697 - val_loss: 1.6474 - val_acc: 0.7200\n",
      "Epoch 34/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.1474 - acc: 0.9428 - val_loss: 1.5988 - val_acc: 0.6987\n",
      "Epoch 35/40\n",
      "1818/1818 [==============================] - 9s 5ms/sample - loss: 0.1089 - acc: 0.9593 - val_loss: 1.1813 - val_acc: 0.7707\n",
      "Epoch 36/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.0922 - acc: 0.9697 - val_loss: 1.2593 - val_acc: 0.7733\n",
      "Epoch 37/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.0683 - acc: 0.9802 - val_loss: 1.3773 - val_acc: 0.7440\n",
      "Epoch 38/40\n",
      "1818/1818 [==============================] - 10s 5ms/sample - loss: 0.0972 - acc: 0.9620 - val_loss: 1.2866 - val_acc: 0.7760\n",
      "Epoch 39/40\n",
      "1818/1818 [==============================] - 9s 5ms/sample - loss: 0.2090 - acc: 0.9285 - val_loss: 1.4646 - val_acc: 0.7067\n",
      "Epoch 40/40\n",
      "1818/1818 [==============================] - 9s 5ms/sample - loss: 0.1899 - acc: 0.9356 - val_loss: 1.2796 - val_acc: 0.7813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c58e98ec8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_images, train_labels, epochs= 40, validation_data= (validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1818/1818 [==============================] - 2s 1ms/sample - loss: 0.1093 - acc: 0.9609\n",
      "Train Accuracy0.96 \n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = cnn_model.evaluate(train_images, train_labels) \n",
    "print('Train Accuracy{} '.format(round(float(train_accuracy), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 1ms/sample - loss: 1.2796 - acc: 0.7813\n",
      "Validation Accuracy0.78 \n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_accuracy = cnn_model.evaluate(validation_images, validation_labels) \n",
    "print('Validation Accuracy{} '.format(round(float(validation_accuracy), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 1ms/sample - loss: 1.2120 - acc: 0.7252\n",
      "Test Accuracy 0.73 \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = cnn_model.evaluate(test_images, test_labels)\n",
    "print('Test Accuracy {} '.format(round(float(test_accuracy), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, display1, display2):\n",
    "    \"\"\"\n",
    "    Reads an image from testing dataset, and predict its label.\n",
    "    \n",
    "    Returns the image label and testing accuracy. \n",
    "    \"\"\"\n",
    "    \n",
    "    img = np.array([img])\n",
    "\n",
    "    img =img.reshape(1, 28, 28, 1)\n",
    "    predictions = cnn_model.predict(img)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    \n",
    "    display1.setText(\"This image most likely belongs to {}\".format(class_names[np.argmax(predictions)]))\n",
    "    display2.setText(\"Test Accuracy: {} %\".format(round(float(test_accuracy * 100), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplicationWindow(QtWidgets.QMainWindow):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ApplicationWindow, self).__init__()\n",
    "        self.ui = Ui_MainWindow()\n",
    "        self.ui.setupUi(self)\n",
    "        \n",
    "        self.ui.browse_pushButton.clicked.connect(lambda: self.browse(self.ui.org_img_disp_label))\n",
    "        \n",
    "        \n",
    "    def browse(self, Ui_MainWindow):\n",
    "        \"\"\"\n",
    "        Uploads an image, resizes it for prediction,\n",
    "        then resizes it again for plotting. \n",
    "        \"\"\"\n",
    "\n",
    "        filename = QtWidgets.QFileDialog.getOpenFileName(self, 'Open file', \"*.tif\")\n",
    "        if (filename == ([], '')) | (filename ==  0 ):\n",
    "            return\n",
    "        imgPath = str(filename[0])\n",
    "        try:\n",
    "            img = cv2.imread(imgPath, cv2.IMREAD_GRAYSCALE)\n",
    "            img_test = cv2.resize(img, (28, 28))\n",
    "            img_show = cv2.resize(img, (721, 401))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(' ')\n",
    "\n",
    "        self.showImage(img_show, Ui_MainWindow)\n",
    "        predict_image(img_test, self.ui.result_label, self.ui.result_label_2)\n",
    "        \n",
    "        \n",
    "    def showImage(self, arr, Ui_MainWindow):\n",
    "        \"\"\"\n",
    "        Convert an array to image and display it given image handler\n",
    "        \"\"\"\n",
    "\n",
    "        imgBack = Image.fromarray(arr)\n",
    "        qimage = PIL.ImageQt.ImageQt(imgBack)\n",
    "        pixmap = QtGui.QPixmap.fromImage(qimage) \n",
    "        Ui_MainWindow.setPixmap(pixmap)\n",
    "        Ui_MainWindow.setMask(pixmap.mask())\n",
    "        Ui_MainWindow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    application = ApplicationWindow()\n",
    "    application.show()\n",
    "    app.exec_()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
